{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import shap\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on log spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "base_data_path = r'C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms'\n",
    "train_data_path = base_data_path + r\"\\train\"\n",
    "valid_data_path = base_data_path + r\"\\valid\"\n",
    "test_data_path = base_data_path + r\"\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_stats(train_data_path, stats_file):\n",
    "    print(\"Calculating dataset statistics for normalization...\")\n",
    "    all_pixels = []\n",
    "    for root, dirs, files in os.walk(train_data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                img_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = Image.open(img_path).convert(\"RGB\")\n",
    "                    all_pixels.append(np.array(image) / 255.0)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping invalid image: {img_path}\")\n",
    "                    continue\n",
    "    if not all_pixels:\n",
    "        raise ValueError(\"No valid images found in the dataset.\")\n",
    "    all_pixels = np.concatenate([img.reshape(-1, 3) for img in all_pixels], axis=0)\n",
    "    mean = all_pixels.mean(axis=0)\n",
    "    std = all_pixels.std(axis=0)\n",
    "\n",
    "    # Save statistics to a file\n",
    "    with open(stats_file, \"w\") as f:\n",
    "        json.dump({\"mean\": mean.tolist(), \"std\": std.tolist()}, f)\n",
    "\n",
    "    print(f\"Mean: {mean}, Std: {std} (calculated and saved)\")\n",
    "    return mean, std\n",
    "\n",
    "def load_stats(stats_file):\n",
    "    with open(stats_file, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    mean = np.array(stats[\"mean\"])\n",
    "    std = np.array(stats[\"std\"])\n",
    "    print(f\"Mean: {mean}, Std: {std} (loaded from file)\")\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_file = base_data_path + r\"\\metrics.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.01005489 0.01015358 0.01138446], Std: [0.0485352  0.04866005 0.04982412] (loaded from file)\n"
     ]
    }
   ],
   "source": [
    "# Load\\Calculate mean and std dynamically \n",
    "\n",
    "if os.path.exists(stats_file):\n",
    "    # Load saved statistics\n",
    "    mean, std = load_stats(stats_file)\n",
    "else:\n",
    "    # Calculate and save statistics\n",
    "    # Approximately 3 minutes for the entire training set (~50'000 images)\n",
    "    mean, std = calculate_and_save_stats(train_data_path, stats_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset\n",
    "'''\n",
    "class EarthquakeDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.file_list = [f for f in os.listdir(data_path) if f.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.file_list[idx]\n",
    "        img_path = os.path.join(self.data_path, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Extract label from filename\n",
    "        if \"_post.png\" in img_name:\n",
    "            label = 1  # Aftershock\n",
    "        elif \"_pre.png\" in img_name:\n",
    "            label = 0  # Mainshock\n",
    "        else:\n",
    "            raise ValueError(\"Filename does not match expected pattern\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class EarthquakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.valid_files = []\n",
    "\n",
    "        # Verify files during initialization\n",
    "        for root, dirs, files in os.walk(data_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".png\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        # Attempt to open the file to verify it's valid\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img.verify()  # Check if the file is a valid image\n",
    "                        self.valid_files.append(file_path)  # Add valid files\n",
    "                    except (UnidentifiedImageError, OSError):\n",
    "                        print(f\"Skipping invalid file: {file_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.valid_files[idx]  # Use self.valid_files\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Extract label from filename\n",
    "        if \"_post.png\" in img_path:\n",
    "            label = 1  # Aftershock\n",
    "        elif \"_pre.png\" in img_path:\n",
    "            label = 0  # Mainshock\n",
    "        else:\n",
    "            raise ValueError(\"Filename does not match expected pattern\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\FDMO.IV.100017451_EV_pre.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\MMO1.IV.100364611_EV_post.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\T1212.IV.100399621_EV_post.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\T1212.IV.100401452_EV_post.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\T1214.IV.100019705_EV_pre.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\T1216.IV.100099881_EV_post.png\n",
      "Skipping invalid file: C:\\Users\\frmar\\OneDrive\\Desktop\\EQML Project\\data_preprocessed\\data_preprocessed\\spectrograms\\train\\T1244.IV.100291054_EV_post.png\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and dataloaders\n",
    "dataset = EarthquakeDataset(train_data_path, transform=transform)\n",
    "valid_dataset = EarthquakeDataset(valid_data_path, transform=transform)\n",
    "test_dataset = EarthquakeDataset(test_data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0) \n",
    "# Dataloaders batches are shaped (32, 3, 33, 153)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN and training utils function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Update fc1 to reflect the correct input size after convolution and pooling\n",
    "        self.fc1 = nn.Linear(128 * 4 * 19, 128)  # Adjusted based on H=33, W=153\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.maxpool(self.BatchNorm1(self.conv1(x))))\n",
    "        x = self.relu(self.maxpool(self.BatchNorm2(self.conv2(x))))\n",
    "        x = self.relu(self.BatchNorm3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNN2D(num_classes=2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(model, dataloader, dataset_name):\n",
    "    print(f\"Calculating accuracy on {dataset_name} set...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, valid_dataloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        total_batches = len(dataloader)  # Total number of batches in the training set\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate the percentage of the training set trained on\n",
    "            percent_trained = (batch_idx + 1) / total_batches * 100\n",
    "\n",
    "            # Print progress on the same line\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{total_batches} - {percent_trained:.2f}% of training set trained\", end='\\r')\n",
    "            sys.stdout.flush()  # Force the output to be updated immediately\n",
    "\n",
    "        print()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}\")\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        train_accuracy = calculate_accuracy(model, dataloader, \"Training\")\n",
    "        valid_accuracy = calculate_accuracy(model, valid_dataloader, \"Validation\")\n",
    "\n",
    "        print(f\"Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\\Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 1/10, Loss: 32737.7914\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:17<29:38, 197.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7505, Valid Accuracy: 0.7448\n",
      "Epoch 2/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 2/10, Loss: 23781.8187\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [06:33<26:13, 196.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8224, Valid Accuracy: 0.8075\n",
      "Epoch 3/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 3/10, Loss: 18202.6219\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [09:47<22:46, 195.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8718, Valid Accuracy: 0.8457\n",
      "Epoch 4/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 4/10, Loss: 15036.6674\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [13:01<19:30, 195.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9132, Valid Accuracy: 0.8725\n",
      "Epoch 5/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 5/10, Loss: 12917.3433\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [16:14<16:10, 194.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9052, Valid Accuracy: 0.8622\n",
      "Epoch 6/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 6/10, Loss: 11064.6095\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [19:25<12:52, 193.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9288, Valid Accuracy: 0.8745\n",
      "Epoch 7/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 7/10, Loss: 9820.0677\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [22:38<09:39, 193.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9415, Valid Accuracy: 0.8746\n",
      "Epoch 8/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 8/10, Loss: 8398.1046\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [25:54<06:28, 194.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9504, Valid Accuracy: 0.8773\n",
      "Epoch 9/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 9/10, Loss: 7359.1496\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [29:11<03:15, 195.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9524, Valid Accuracy: 0.8743\n",
      "Epoch 10/10, Batch 1738/1738 - 100.00% of training set trained\n",
      "Epoch 10/10, Loss: 6205.3311\n",
      "Calculating accuracy on Training set...\n",
      "Calculating accuracy on Validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [32:25<00:00, 194.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9611, Valid Accuracy: 0.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, dataloader, valid_dataloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), r\"trained_models\\cnn_non_log.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded and ready for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frmar\\AppData\\Local\\Temp\\ipykernel_4800\\3847499948.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"trained_models\\cnn_non_log.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = CNN2D(num_classes=2).to(device)\n",
    "model.load_state_dict(torch.load(r\"trained_models\\cnn_non_log.pth\", map_location=device))\n",
    "model.eval()\n",
    "print(\"Model successfully loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Mainshock       0.86      0.89      0.87      5810\n",
      "  Aftershock       0.89      0.86      0.87      6109\n",
      "\n",
      "    accuracy                           0.87     11919\n",
      "   macro avg       0.87      0.87      0.87     11919\n",
      "weighted avg       0.87      0.87      0.87     11919\n",
      "\n",
      "\n",
      "Test Accuracy: 0.8742\n",
      "Precision: 0.8935\n",
      "Recall: 0.8566\n",
      "F1 Score: 0.8746\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_data_path = base_data_path + r\"\\test\"\n",
    "test_dataset = EarthquakeDataset(test_data_path, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Mainshock', 'Aftershock']))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (cm[0,0] + cm[1,1]) / np.sum(cm)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "precision = cm[1,1] / (cm[0,1] + cm[1,1])\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background shape: torch.Size([16, 3, 33, 153])\n",
      "Test images shape: torch.Size([16, 3, 33, 153])\n",
      "Model output shape: torch.Size([16, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (19) must match the size of tensor b (128) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel output shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model(background)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m e \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model, background)\n\u001b[1;32m---> 26\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m shap_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mtranspose(shap_values, (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     30\u001b[0m test_numpy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(np\u001b[38;5;241m.\u001b[39mswapaxes(test_images\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:159\u001b[0m, in \u001b[0;36mDeepExplainer.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, ranked_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_additivity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_additivity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:186\u001b[0m, in \u001b[0;36mPyTorchDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    185\u001b[0m feature_ind \u001b[38;5;241m=\u001b[39m model_output_ranks[j, i]\n\u001b[1;32m--> 186\u001b[0m sample_phis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterim:\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:119\u001b[0m, in \u001b[0;36mPyTorchDeep.gradient\u001b[1;34m(self, idx, inputs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m--> 119\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m             grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\torch\\utils\\hooks.py:139\u001b[0m, in \u001b[0;36mBackwardHook._set_user_hook.<locals>.hook\u001b[1;34m(grad_input, _)\u001b[0m\n\u001b[0;32m    136\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pack_with_none(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_tensors_index, grad_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_inputs)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_hooks:\n\u001b[1;32m--> 139\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:241\u001b[0m, in \u001b[0;36mdeeplift_grad\u001b[1;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;129;01min\u001b[39;00m op_handler:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op_handler[module_type]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear_1d\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m--> 241\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop_handler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized nn.Module: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\frmar\\OneDrive\\Desktop\\GitHub\\CNN-explainability-Earthquakes\\CNN_EQML\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_pytorch.py:351\u001b[0m, in \u001b[0;36mnonlinear_1d\u001b[1;34m(module, grad_input, grad_output)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# handles numerical instabilities where delta_in is very small by\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# just taking the gradient in those cases\u001b[39;00m\n\u001b[0;32m    349\u001b[0m grads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m grad_input]\n\u001b[0;32m    350\u001b[0m grads[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(torch\u001b[38;5;241m.\u001b[39mabs(delta_in\u001b[38;5;241m.\u001b[39mrepeat(dup0)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m, grad_input[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 351\u001b[0m                        \u001b[43mgrad_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_in\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdup0\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(grads)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (19) must match the size of tensor b (128) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# since shuffle=True, this is a random sample of test data\n",
    "batch = next(iter(test_dataloader))\n",
    "images, _ = batch\n",
    "\n",
    "background = images[:16]\n",
    "test_images = images[16:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "background = background.to(device)\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "print(\"Background shape:\", background.shape)\n",
    "print(\"Test images shape:\", test_images.shape)\n",
    "print(\"Model output shape:\", model(background).shape)\n",
    "\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(test_images)\n",
    "\n",
    "\n",
    "shap_numpy = list(np.transpose(shap_values, (4, 0, 2, 3, 1)))\n",
    "test_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)\n",
    "\n",
    "# plot the feature attributions\n",
    "shap.image_plot(shap_numpy, -test_numpy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN_EQML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
